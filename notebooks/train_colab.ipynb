{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"id": "view-in-github"},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AhmetSeyhan/scanner-ultra/blob/main/notebooks/train_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Scanner ULTRA v5.0 ‚Äî PentaShield‚Ñ¢ Model Training\n",
    "\n",
    "**Google Colab Enterprise / Colab Pro+ Training Notebook**\n",
    "\n",
    "Bu notebook, Scanner ULTRA deepfake detection modellerini eƒüitmek i√ßin tasarlanmƒ±≈ütƒ±r.\n",
    "\n",
    "## Modeller\n",
    "| Model | Params | Dataset | Hedef AUC |\n",
    "|-------|--------|---------|----------|\n",
    "| EfficientNet-B0 | 5.3M | FF++ | >0.95 |\n",
    "| CLIP ViT-L/14 (LayerNorm) | 428M (0.03% trainable) | FF++ | >0.93 |\n",
    "| Xception | 22M | FF++ | >0.96 |\n",
    "| ViT-B/16 | 86M | FF++ | >0.94 |\n",
    "\n",
    "## GPU Gereksinimleri\n",
    "- **Minimum:** T4 (16GB) ‚Äî EfficientNet + CLIP\n",
    "- **√ñnerilen:** A100 (40GB) ‚Äî T√ºm modeller paralel\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ GPU & Ortam Kontrol√º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "# GPU kontrol√º\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version',\n",
    "                         '--format=csv,noheader'], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print('‚úÖ GPU Bulundu:', result.stdout.strip())\n",
    "else:\n",
    "    print('‚ö†Ô∏è  GPU bulunamadƒ± ‚Äî CPU ile devam edilecek (YAVA≈û)')\n",
    "\n",
    "# Python & CUDA\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.version.cuda}')\n",
    "print(f'Device: {\"cuda\" if torch.cuda.is_available() else \"cpu\"}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Google Drive Baƒülantƒ±sƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "# √áalƒ±≈üma dizinleri\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/Scanner'\n",
    "os.makedirs(f'{DRIVE_ROOT}/weights', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_ROOT}/datasets', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_ROOT}/logs', exist_ok=True)\n",
    "print(f'‚úÖ Drive baƒülandƒ±: {DRIVE_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Repo Clone & Baƒüƒ±mlƒ±lƒ±klar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Repo klonla\n",
    "if [ ! -d '/content/scanner-ultra' ]; then\n",
    "    git clone https://github.com/AhmetSeyhan/scanner-ultra.git /content/scanner-ultra\n",
    "    echo '‚úÖ Repo klonlandƒ±'\n",
    "else\n",
    "    cd /content/scanner-ultra && git pull\n",
    "    echo '‚úÖ Repo g√ºncellendi'\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Baƒüƒ±mlƒ±lƒ±klarƒ± kur\n",
    "pip install -q \\\n",
    "    timm>=0.9.12 \\\n",
    "    transformers>=4.36.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    librosa>=0.10.0 \\\n",
    "    soundfile>=0.12.0 \\\n",
    "    opencv-python-headless>=4.9.0 \\\n",
    "    scikit-learn>=1.3.0 \\\n",
    "    matplotlib>=3.7.0 \\\n",
    "    tqdm>=4.66.0\n",
    "echo '‚úÖ Baƒüƒ±mlƒ±lƒ±klar kuruldu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/scanner-ultra/src')\n",
    "sys.path.insert(0, '/content/scanner-ultra/scripts/training')\n",
    "print('‚úÖ Path ayarlandƒ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Dataset Hazƒ±rlama\n",
    "\n",
    "### Se√ßenek A: FaceForensics++ (√ñnerilen)\n",
    "### Se√ßenek B: Celeb-DF v2\n",
    "### Se√ßenek C: Demo Dataset (K√º√ß√ºk test i√ßin)\n",
    "\n",
    "> **NOT:** FaceForensics++ akademik lisans gerektirir: https://github.com/ondyari/FaceForensics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SE√áENEK C: Demo Dataset (Hƒ±zlƒ± test ‚Äî ger√ßek veri yok)\n",
    "# Kendi verini kullanmak i√ßin SE√áENEK A veya B'yi kullan\n",
    "# ============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "DATASET_DIR = '/content/demo_dataset'\n",
    "USE_DEMO = True  # Ger√ßek veri varsa False yap\n",
    "\n",
    "if USE_DEMO:\n",
    "    print('üìÅ Demo dataset olu≈üturuluyor...')\n",
    "    for split in ['real', 'fake']:\n",
    "        os.makedirs(f'{DATASET_DIR}/{split}', exist_ok=True)\n",
    "    \n",
    "    # 200 sahte g√∂r√ºnt√º olu≈ütur (ger√ßek eƒüitimde ger√ßek veri kullan!)\n",
    "    for i in range(100):\n",
    "        # Real: d√ºz renk + g√ºr√ºlt√º\n",
    "        img = Image.fromarray(\n",
    "            (np.random.rand(224, 224, 3) * 255).astype(np.uint8))\n",
    "        img.save(f'{DATASET_DIR}/real/img_{i:04d}.jpg')\n",
    "        \n",
    "        # Fake: farklƒ± daƒüƒ±lƒ±m\n",
    "        img = Image.fromarray(\n",
    "            (np.random.rand(224, 224, 3) * 128 + 64).astype(np.uint8))\n",
    "        img.save(f'{DATASET_DIR}/fake/img_{i:04d}.jpg')\n",
    "    \n",
    "    print(f'‚úÖ Demo dataset: {DATASET_DIR}')\n",
    "    print('   real/: 100 g√∂r√ºnt√º, fake/: 100 g√∂r√ºnt√º')\n",
    "    print('   ‚ö†Ô∏è  Bu DEMO verisi ‚Äî ger√ßek eƒüitim i√ßin FF++ kullan!')\n",
    "\n",
    "else:\n",
    "    # Ger√ßek dataset yolu\n",
    "    # Google Drive'dan kopyala:\n",
    "    # !cp -r '/content/drive/MyDrive/Scanner/datasets/FF++' /content/dataset\n",
    "    DATASET_DIR = '/content/dataset'  # Buraya ger√ßek yolu gir\n",
    "    print(f'Dataset: {DATASET_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SE√áENEK A: FaceForensics++ ƒ∞ndirme\n",
    "# (download_script.py gerekli ‚Äî FF++ sitesinden al)\n",
    "# ============================================================\n",
    "# FF++ video'larƒ±ndan frame √ßƒ±karma\n",
    "\n",
    "def extract_ff_frames(video_dir, output_dir, n_frames=10):\n",
    "    \"\"\"FF++ video klas√∂r√ºnden frame √ßƒ±kar.\"\"\"\n",
    "    import cv2\n",
    "    from pathlib import Path\n",
    "    \n",
    "    video_dir = Path(video_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    count = 0\n",
    "    for video_path in video_dir.rglob('*.mp4'):\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, total-1, n_frames, dtype=int)\n",
    "        \n",
    "        for fi in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, fi)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                out_path = output_dir / f'{video_path.stem}_f{fi:06d}.jpg'\n",
    "                cv2.imwrite(str(out_path), frame)\n",
    "                count += 1\n",
    "        cap.release()\n",
    "    \n",
    "    print(f'‚úÖ {count} frame √ßƒ±karƒ±ldƒ± ‚Üí {output_dir}')\n",
    "    return count\n",
    "\n",
    "# Kullanƒ±m:\n",
    "# extract_ff_frames('/content/drive/MyDrive/FF++/original_sequences/youtube/c23/videos', \n",
    "#                   '/content/dataset/real')\n",
    "# extract_ff_frames('/content/drive/MyDrive/FF++/manipulated_sequences/Deepfakes/c23/videos',\n",
    "#                   '/content/dataset/fake')\n",
    "\n",
    "print('Frame √ßƒ±karma fonksiyonu hazƒ±r (FF++ i√ßin)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ EfficientNet-B0 Eƒüitimi (Ana Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time, logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s ‚Äî %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    REAL_DIRS = {'real', 'authentic', 'genuine', 'original'}\n",
    "    FAKE_DIRS = {'fake', 'deepfake', 'manipulated', 'synthetic', 'generated'}\n",
    "    IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}\n",
    "\n",
    "    def __init__(self, data_dir, transform=None, split='train', val_ratio=0.15, seed=42):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for subdir in Path(data_dir).iterdir():\n",
    "            if not subdir.is_dir(): continue\n",
    "            name = subdir.name.lower()\n",
    "            if name in self.REAL_DIRS: label = 0\n",
    "            elif name in self.FAKE_DIRS: label = 1\n",
    "            else: continue\n",
    "            self.samples += [(p, label) for p in subdir.rglob('*')\n",
    "                             if p.suffix.lower() in self.IMG_EXTS]\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(len(self.samples)).tolist()\n",
    "        n_val = int(len(idx) * val_ratio)\n",
    "        self.samples = [self.samples[i] for i in (idx[:n_val] if split == 'val' else idx[n_val:])]\n",
    "        print(f'{split}: {len(self.samples)} samples '\n",
    "              f'(real={sum(1 for _,l in self.samples if l==0)}, '\n",
    "              f'fake={sum(1 for _,l in self.samples if l==1)})')\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        path, label = self.samples[i]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return self.transform(img) if self.transform else img, label\n",
    "\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = DeepfakeDataset(DATASET_DIR, train_tf, 'train')\n",
    "val_ds   = DeepfakeDataset(DATASET_DIR, val_tf, 'val')\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "print('‚úÖ DataLoader hazƒ±r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=2)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "\n",
    "EPOCHS = 20  # Ger√ßek veri i√ßin artƒ±r\n",
    "WEIGHTS_OUT = f'{DRIVE_ROOT}/weights'\n",
    "\n",
    "print(f'Model: EfficientNet-B0 | Params: {sum(p.numel() for p in model.parameters())/1e6:.1f}M')\n",
    "print(f'Epochs: {EPOCHS} | Output: {WEIGHTS_OUT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(imgs), labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(imgs)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    for imgs, labels in loader:\n",
    "        logits = model(imgs.to(DEVICE))\n",
    "        probs = F.softmax(logits, dim=-1)[:, 1].cpu().numpy()\n",
    "        preds = logits.argmax(-1).cpu().numpy()\n",
    "        all_preds.extend(preds); all_labels.extend(labels.numpy()); all_probs.extend(probs)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0\n",
    "    f1  = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    return acc, auc, f1\n",
    "\n",
    "\n",
    "best_auc = 0.0\n",
    "history = []\n",
    "\n",
    "print('üöÄ EfficientNet-B0 Eƒüitimi Ba≈ülƒ±yor...')\n",
    "print('-' * 65)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    acc, auc, f1 = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    history.append({'epoch': epoch, 'loss': loss, 'acc': acc, 'auc': auc, 'f1': f1})\n",
    "    print(f'Epoch {epoch:3d}/{EPOCHS} | loss={loss:.4f} acc={acc:.4f} auc={auc:.4f} f1={f1:.4f} | {elapsed:.0f}s')\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(model.state_dict(), f'{WEIGHTS_OUT}/best_efficientnet_b0.pth')\n",
    "        print(f'  *** ‚úÖ Yeni Best AUC={best_auc:.4f} ‚Äî Drive\\'e kaydedildi ***')\n",
    "\n",
    "torch.save(model.state_dict(), f'{WEIGHTS_OUT}/last_efficientnet_b0.pth')\n",
    "print(f'\\nüéâ Eƒüitim tamamlandƒ±! Best AUC={best_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eƒüitim Grafiƒüi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "epochs = [h['epoch'] for h in history]\n",
    "\n",
    "axes[0].plot(epochs, [h['loss'] for h in history], 'b-o', markersize=3)\n",
    "axes[0].set_title('Training Loss'); axes[0].set_xlabel('Epoch'); axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(epochs, [h['auc'] for h in history], 'r-o', markersize=3)\n",
    "axes[1].set_title('Val AUC'); axes[1].set_xlabel('Epoch'); axes[1].grid(True)\n",
    "axes[1].axhline(0.95, color='g', linestyle='--', label='Hedef 0.95')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(epochs, [h['acc'] for h in history], 'g-o', markersize=3)\n",
    "axes[2].plot(epochs, [h['f1'] for h in history], 'm-o', markersize=3)\n",
    "axes[2].set_title('Accuracy & F1'); axes[2].set_xlabel('Epoch')\n",
    "axes[2].legend(['Accuracy', 'F1']); axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DRIVE_ROOT}/logs/efficientnet_training.png', dpi=150)\n",
    "plt.show()\n",
    "print('üìä Grafik kaydedildi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ CLIP ViT-L/14 Fine-Tuning (LayerNorm-Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "print('üì• CLIP ViT-L/14 y√ºkleniyor (428M parametre)...')\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "\n",
    "# Freeze all ‚Äî unfreeze only LayerNorm\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_trainable = 0\n",
    "for name, param in clip_model.named_parameters():\n",
    "    if 'layernorm' in name.lower() or 'layer_norm' in name.lower():\n",
    "        param.requires_grad = True\n",
    "        n_trainable += param.numel()\n",
    "\n",
    "total_params = sum(p.numel() for p in clip_model.parameters())\n",
    "print(f'‚úÖ CLIP y√ºklendi')\n",
    "print(f'   Total: {total_params/1e6:.1f}M params')\n",
    "print(f'   Trainable (LayerNorm): {n_trainable:,} ({100*n_trainable/total_params:.3f}%)')\n",
    "\n",
    "clip_model = clip_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe head (CLIP √ºst√ºne binary classifier)\n",
    "clip_probe = nn.Sequential(\n",
    "    nn.Linear(768, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, 2)\n",
    ").to(DEVICE)\n",
    "\n",
    "def clip_collate(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    inputs = clip_processor(images=list(imgs), return_tensors='pt', padding=True)\n",
    "    return inputs, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Raw PIL images i√ßin dataset\n",
    "class RawPILDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        path, label = self.samples[i]\n",
    "        return Image.open(path).convert('RGB'), label\n",
    "\n",
    "train_ds2 = DeepfakeDataset(DATASET_DIR, transform=None, split='train')\n",
    "val_ds2   = DeepfakeDataset(DATASET_DIR, transform=None, split='val')\n",
    "# transform=None i√ßin PIL d√∂nd√ºrmek √ºzere __getitem__ g√ºncellenmeli\n",
    "# (Demo dataset zaten PIL-compatible)\n",
    "\n",
    "clip_train = DataLoader(\n",
    "    [(Image.open(p).convert('RGB'), l) for p, l in train_ds2.samples[:500]],  # ƒ∞lk 500\n",
    "    batch_size=16, shuffle=True, collate_fn=clip_collate\n",
    ")\n",
    "\n",
    "clip_val = DataLoader(\n",
    "    [(Image.open(p).convert('RGB'), l) for p, l in val_ds2.samples[:100]],\n",
    "    batch_size=16, shuffle=False, collate_fn=clip_collate\n",
    ")\n",
    "\n",
    "print(f'‚úÖ CLIP DataLoader: train={len(clip_train.dataset)}, val={len(clip_val.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_params = (list(filter(lambda p: p.requires_grad, clip_model.parameters()))\n",
    "               + list(clip_probe.parameters()))\n",
    "clip_optimizer = optim.AdamW(clip_params, lr=5e-5, weight_decay=1e-4)\n",
    "CLIP_EPOCHS = 10\n",
    "\n",
    "best_clip_auc = 0.0\n",
    "print('üöÄ CLIP Fine-tuning Ba≈ülƒ±yor...')\n",
    "\n",
    "for epoch in range(1, CLIP_EPOCHS + 1):\n",
    "    # Train\n",
    "    clip_model.train(); clip_probe.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in clip_train:\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        labels = labels.to(DEVICE)\n",
    "        clip_optimizer.zero_grad()\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features = features / (features.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        loss = criterion(clip_probe(features), labels)\n",
    "        loss.backward()\n",
    "        clip_optimizer.step()\n",
    "        total_loss += loss.item() * len(labels)\n",
    "    \n",
    "    # Eval\n",
    "    clip_model.eval(); clip_probe.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in clip_val:\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = features / (features.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "            logits = clip_probe(features)\n",
    "            probs = F.softmax(logits, -1)[:, 1].cpu().numpy()\n",
    "            all_preds.extend(logits.argmax(-1).cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Epoch {epoch:2d}/{CLIP_EPOCHS} | loss={total_loss/len(clip_train.dataset):.4f} '\n",
    "          f'acc={acc:.4f} auc={auc:.4f}')\n",
    "    \n",
    "    if auc > best_clip_auc:\n",
    "        best_clip_auc = auc\n",
    "        # LayerNorm aƒüƒ±rlƒ±klarƒ±nƒ± kaydet (k√º√ß√ºk dosya ~2MB)\n",
    "        ln_state = {k: v for k, v in clip_model.state_dict().items()\n",
    "                    if 'layernorm' in k.lower() or 'layer_norm' in k.lower()}\n",
    "        torch.save(ln_state, f'{WEIGHTS_OUT}/clip_layernorm.pth')\n",
    "        torch.save(clip_probe.state_dict(), f'{WEIGHTS_OUT}/clip_probe_head.pth')\n",
    "        print(f'  *** ‚úÖ Best CLIP AUC={best_clip_auc:.4f} kaydedildi ***')\n",
    "\n",
    "print(f'\\nüéâ CLIP Fine-tuning tamamlandƒ±! Best AUC={best_clip_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Xception Eƒüitimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception (pretrained via timm)\n",
    "print('üì• Xception y√ºkleniyor...')\n",
    "xception = timm.create_model('xception', pretrained=True, num_classes=2).to(DEVICE)\n",
    "\n",
    "xception_optimizer = optim.AdamW(xception.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "xception_scheduler = optim.lr_scheduler.CosineAnnealingLR(xception_optimizer, T_max=15)\n",
    "\n",
    "xception_tf = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Xception 299x299 ister\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Xception norm\n",
    "])\n",
    "xception_val_tf = transforms.Compose([\n",
    "    transforms.Resize((299, 299)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "xc_train_ds = DeepfakeDataset(DATASET_DIR, xception_tf, 'train')\n",
    "xc_val_ds   = DeepfakeDataset(DATASET_DIR, xception_val_tf, 'val')\n",
    "xc_train_loader = DataLoader(xc_train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
    "xc_val_loader   = DataLoader(xc_val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "XCEPTION_EPOCHS = 15\n",
    "best_xc_auc = 0.0\n",
    "print('üöÄ Xception Eƒüitimi Ba≈ülƒ±yor...')\n",
    "\n",
    "for epoch in range(1, XCEPTION_EPOCHS + 1):\n",
    "    loss = train_one_epoch(xception, xc_train_loader, criterion, xception_optimizer)\n",
    "    acc, auc, f1 = evaluate(xception, xc_val_loader)\n",
    "    xception_scheduler.step()\n",
    "    print(f'Epoch {epoch:2d}/{XCEPTION_EPOCHS} | loss={loss:.4f} acc={acc:.4f} auc={auc:.4f}')\n",
    "    if auc > best_xc_auc:\n",
    "        best_xc_auc = auc\n",
    "        torch.save(xception.state_dict(), f'{WEIGHTS_OUT}/best_xception.pth')\n",
    "        print(f'  *** ‚úÖ Best Xception AUC={best_xc_auc:.4f} ***')\n",
    "\n",
    "print(f'\\nüéâ Xception tamamlandƒ±! Best AUC={best_xc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Sonu√ßlar & Model Kar≈üƒ±la≈ütƒ±rmasƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    {'Model': 'EfficientNet-B0', 'Best AUC': best_auc, 'Params': '5.3M', 'File': 'best_efficientnet_b0.pth'},\n",
    "    {'Model': 'CLIP ViT-L/14 (LN-only)', 'Best AUC': best_clip_auc, 'Params': '0.03%', 'File': 'clip_layernorm.pth'},\n",
    "    {'Model': 'Xception', 'Best AUC': best_xc_auc, 'Params': '22M', 'File': 'best_xception.pth'},\n",
    "]\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üìä Eƒûƒ∞Tƒ∞M SONU√áLARI')\n",
    "print('='*60)\n",
    "for r in results:\n",
    "    status = '‚úÖ' if r['Best AUC'] > 0.9 else '‚ö†Ô∏è '\n",
    "    print(f\"{status} {r['Model']:30s} | AUC={r['Best AUC']:.4f} | Params={r['Params']}\")\n",
    "print('='*60)\n",
    "print(f'\\nüìÅ Aƒüƒ±rlƒ±klar kaydedildi: {WEIGHTS_OUT}')\n",
    "import os\n",
    "for f in os.listdir(WEIGHTS_OUT):\n",
    "    size = os.path.getsize(f'{WEIGHTS_OUT}/{f}') / 1e6\n",
    "    print(f'   {f}: {size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Modeli Scanner ULTRA'ya Entegre Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scanner ULTRA'ya y√ºkleyip test et\n",
    "import sys\n",
    "sys.path.insert(0, '/content/scanner-ultra/src')\n",
    "\n",
    "import asyncio\n",
    "from scanner.core.visual.efficientnet_detector import EfficientNetDetector\n",
    "from scanner.core.visual.clip_detector import CLIPDetector\n",
    "from scanner.core.base_detector import DetectorInput\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "async def test_detectors():\n",
    "    # EfficientNet\n",
    "    eff_det = EfficientNetDetector(\n",
    "        model_path=f'{WEIGHTS_OUT}/best_efficientnet_b0.pth'\n",
    "    )\n",
    "    await eff_det.load_model()\n",
    "    \n",
    "    # Test frame\n",
    "    test_frame = (np.random.rand(224, 224, 3) * 255).astype(np.uint8)\n",
    "    inp = DetectorInput(frames=[test_frame])\n",
    "    result = await eff_det.detect(inp)\n",
    "    print(f'EfficientNet: score={result.score:.4f}, confidence={result.confidence:.4f}, method={result.method}')\n",
    "    \n",
    "    # CLIP\n",
    "    clip_det = CLIPDetector(\n",
    "        model_path=f'{WEIGHTS_OUT}/clip_layernorm.pth'\n",
    "    )\n",
    "    await clip_det.load_model()\n",
    "    result2 = await clip_det.detect(inp)\n",
    "    print(f'CLIP: score={result2.score:.4f}, confidence={result2.confidence:.4f}, method={result2.method}')\n",
    "    \n",
    "    print('\\n‚úÖ Modeller Scanner ULTRA ile uyumlu!')\n",
    "\n",
    "await test_detectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Storage'a y√ºkle (opsiyonel)\n",
    "# from google.cloud import storage\n",
    "# \n",
    "# def upload_to_gcs(local_path, bucket_name, gcs_path):\n",
    "#     client = storage.Client()\n",
    "#     bucket = client.bucket(bucket_name)\n",
    "#     blob = bucket.blob(gcs_path)\n",
    "#     blob.upload_from_filename(local_path)\n",
    "#     print(f'‚úÖ GCS\\'e y√ºklendi: gs://{bucket_name}/{gcs_path}')\n",
    "# \n",
    "# BUCKET = 'scanner-ultra-weights'  # GCS bucket adƒ±n\n",
    "# for fname in ['best_efficientnet_b0.pth', 'clip_layernorm.pth', 'best_xception.pth']:\n",
    "#     fpath = f'{WEIGHTS_OUT}/{fname}'\n",
    "#     if os.path.exists(fpath):\n",
    "#         upload_to_gcs(fpath, BUCKET, f'weights/v5/{fname}')\n",
    "\n",
    "print('üí° GCS y√ºkleme i√ßin yukarƒ±daki kodu uncomment et')\n",
    "print('   √ñnce: !pip install google-cloud-storage')\n",
    "print('   Sonra: GCS bucket olu≈ütur ve bucket adƒ±nƒ± gir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ √ñzet & Sonraki Adƒ±mlar\n",
    "\n",
    "### Eƒüitilen Modeller\n",
    "- ‚úÖ `best_efficientnet_b0.pth` ‚Äî EfficientNet-B0 binary classifier\n",
    "- ‚úÖ `clip_layernorm.pth` ‚Äî CLIP LayerNorm weights\n",
    "- ‚úÖ `clip_probe_head.pth` ‚Äî CLIP probe head\n",
    "- ‚úÖ `best_xception.pth` ‚Äî Xception binary classifier\n",
    "\n",
    "### Sonraki Adƒ±mlar\n",
    "1. **Daha b√ºy√ºk dataset:** FaceForensics++ tam dataset ile yeniden eƒüit\n",
    "2. **ViT eƒüitimi:** `timm.create_model('vit_base_patch16_224', ...)` \n",
    "3. **WavLM audio:** `scripts/training/train_audio.py` √ßalƒ±≈ütƒ±r\n",
    "4. **API deploy:** `docker-compose up` ile production'a al\n",
    "5. **Vertex AI:** Bu notebook'u Vertex AI Training Job'a d√∂n√º≈üt√ºr\n",
    "\n",
    "### Aƒüƒ±rlƒ±klarƒ± Production'a Aktar\n",
    "```bash\n",
    "# API'ye y√ºkle\n",
    "curl -X POST http://localhost:8000/v1/scan \\\n",
    "  -H 'X-API-Key: your-key' \\\n",
    "  -F 'file=@test_video.mp4' \\\n",
    "  -F 'weights_dir=/app/weights'\n",
    "```"
   ]
  }
 ]
}
